{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning 2024 - Project Assignment\n",
    "This notebook contains the code for the 2024 **project assignment** of the course **Deep Learning** - MSc in Artificial Intelligence Systems - University of Trento.\n",
    "\n",
    "The project is developed by [Alessandro Lorenzi](mailto:alessandro.lorenzi-1@studenti.unitn.it) and [Luca Cazzola](mailto:luca.cazzola-1@studenti.unitn.it).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective of this project is to implement a **Test-Time Adaptation (TTA)** solution, focusing on enhancing the predictive capabilities of pre-trained neural networks on unseen test samples.\n",
    "\n",
    "Throughout this notebook, we implement the **Test-Time Prompt Tuning (TPT)** method in combination also with **CoOp**, a few-shot prompt tuning strategy.\n",
    "\n",
    "Additionally, we experiment with different **image augmentation techniques** to assess their impact on the model's performance.\n",
    "\n",
    "To further improve the effectiveness of our approach, we propose a novel method that utilizes **image captioning to generate more context-aware prompts**. \n",
    "\n",
    "The goal of this notebook is to evaluate these methods and demonstrate improvements in model performance without access to test labels or additional data during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Index\n",
    "\n",
    "To make this notebook easier to navigate, we've created a smart index with links to the most important sections. Each section includes explanations and comments to guide you through the key aspects of our implementation and experiments. Click on the links below to jump directly to the relevant parts of the notebook:\n",
    "\n",
    "- [Image Augmentations](#Image-Augmentations-⭐)\n",
    "- [Prompt-Augmentation](#Prompt-Augmentation-⭐)\n",
    "- [Results obtained with TPT on ImageNet-A](#Results-obtained-with-TPT-on-ImageNet-A-⭐)\n",
    "- [Conclusions](#Conclusions-⭐)\n",
    "- [Augmentations PlayGround](#Augmentations-PlayGround-⭐)\n",
    "\n",
    "Feel free to explore each section to understand our approach and findings in detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C39Q3fh7-lXM"
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEi90DRG-qMI",
    "outputId": "5070d569-c8d1-4383-a7ea-0f64a82923e7"
   },
   "outputs": [],
   "source": [
    "!pip install -q ftfy regex tqdm scikit-learn scikit-image\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "!wget -P /lib https://raw.githubusercontent.com/google-research/augmix/master/augmentations.py\n",
    "\n",
    "!pip install wandb\n",
    "!pip install -U diffusers\n",
    "!pip install keybert[spacy]\n",
    "!pip install -U accelerate\n",
    "!pip install --upgrade accelerate\n",
    "\n",
    "# Core modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset, RandomSampler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "\n",
    "# Models\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "from diffusers import StableDiffusionImageVariationPipeline\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# Plotting\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# AWS\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "# Utility\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
     ]
    }
   ],
   "source": [
    "# Set globally the GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Set clip weights to use\n",
    "print(clip.available_models())\n",
    "clip_version = 'RN50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3J4ssvsKQ8W"
   },
   "source": [
    "## DATASET INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Arguments relative to datasets\n",
    "'''\n",
    "args_data = {\n",
    "    \"dataset\": \"imagenet-a\",   # Dataset of choice\n",
    "    \"n_aug\": 63,               # number of augmentations per image\n",
    "    \"retain_aug_perc\": 0.1,    # percentage of augmentations to retain\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGNET_A_CLASSES = {\n",
    "    \"n01498041\": \"stingray\", \"n01531178\": \"goldfinch\", \"n01534433\": \"junco\", \"n01558993\": \"American robin\", \"n01580077\": \"jay\", \"n01614925\": \"bald eagle\",\"n01616318\": \"vulture\",\"n01631663\": \"newt\",\"n01641577\": \"American bullfrog\",\"n01669191\": \"box turtle\",\"n01677366\": \"green iguana\",\"n01687978\": \"agama\",\"n01694178\": \"chameleon\",\"n01698640\": \"American alligator\",\"n01735189\": \"garter snake\",\"n01770081\": \"harvestman\",\"n01770393\": \"scorpion\",\"n01774750\": \"tarantula\",\"n01784675\": \"centipede\",\"n01819313\": \"sulphur-crested cockatoo\",\"n01820546\": \"lorikeet\",\"n01833805\": \"hummingbird\",\"n01843383\": \"toucan\",\"n01847000\": \"duck\",\"n01855672\": \"goose\",\"n01882714\": \"koala\",\"n01910747\": \"jellyfish\",\"n01914609\": \"sea anemone\",\"n01924916\": \"flatworm\",\"n01944390\": \"snail\",\"n01985128\": \"crayfish\",\"n01986214\": \"hermit crab\",\"n02007558\": \"flamingo\",\"n02009912\": \"great egret\",\"n02037110\": \"oystercatcher\",\"n02051845\": \"pelican\",\"n02077923\": \"sea lion\",\"n02085620\": \"Chihuahua\",\"n02099601\": \"Golden Retriever\",\"n02106550\": \"Rottweiler\",\"n02106662\": \"German Shepherd Dog\",\"n02110958\": \"pug\",\"n02119022\": \"red fox\",\"n02123394\": \"Persian cat\",\"n02127052\": \"lynx\",\"n02129165\": \"lion\",\"n02133161\": \"American black bear\",\"n02137549\": \"mongoose\",\"n02165456\": \"ladybug\",\"n02174001\": \"rhinoceros beetle\",\"n02177972\": \"weevil\",\"n02190166\": \"fly\",\"n02206856\": \"bee\",\"n02219486\": \"ant\",\"n02226429\": \"grasshopper\",\"n02231487\": \"stick insect\",\"n02233338\": \"cockroach\",\"n02236044\": \"mantis\",\"n02259212\": \"leafhopper\",\"n02268443\": \"dragonfly\",\"n02279972\": \"monarch butterfly\",\"n02280649\": \"small white\",\"n02281787\": \"gossamer-winged butterfly\",\"n02317335\": \"starfish\",\"n02325366\": \"cottontail rabbit\",\"n02346627\": \"porcupine\",\"n02356798\": \"fox squirrel\",\"n02361337\": \"marmot\",\"n02410509\": \"bison\",\"n02445715\": \"skunk\",\"n02454379\": \"armadillo\",\"n02486410\": \"baboon\",\"n02492035\": \"white-headed capuchin\",\"n02504458\": \"African bush elephant\",\"n02655020\": \"pufferfish\",\"n02669723\": \"academic gown\",\"n02672831\": \"accordion\",\"n02676566\": \"acoustic guitar\",\"n02690373\": \"airliner\",\"n02701002\": \"ambulance\",\"n02730930\": \"apron\",\"n02777292\": \"balance beam\",\"n02782093\": \"balloon\",\"n02787622\": \"banjo\",\"n02793495\": \"barn\",\"n02797295\": \"wheelbarrow\",\"n02802426\": \"basketball\",\"n02814860\": \"lighthouse\",\"n02815834\": \"beaker\",\"n02837789\": \"bikini\",\"n02879718\": \"bow\",\"n02883205\": \"bow tie\",\"n02895154\": \"breastplate\",\"n02906734\": \"broom\",\"n02948072\": \"candle\",\"n02951358\": \"canoe\",\"n02980441\": \"castle\",\"n02992211\": \"cello\",\"n02999410\": \"chain\",\"n03014705\": \"chest\",\"n03026506\": \"Christmas stocking\",\"n03124043\": \"cowboy boot\",\"n03125729\": \"cradle\",\"n03187595\": \"rotary dial telephone\",\"n03196217\": \"digital clock\",\"n03223299\": \"doormat\",\"n03250847\": \"drumstick\",\"n03255030\": \"dumbbell\",\"n03291819\": \"envelope\",\"n03325584\": \"feather boa\",\"n03355925\": \"flagpole\",\"n03384352\": \"forklift\",\"n03388043\": \"fountain\",\"n03417042\": \"garbage truck\",\"n03443371\": \"goblet\",\"n03444034\": \"go-kart\",\"n03445924\": \"golf cart\",\"n03452741\": \"grand piano\",\"n03483316\": \"hair dryer\",\"n03584829\": \"clothes iron\",\"n03590841\": \"jack-o'-lantern\",\"n03594945\": \"jeep\",\"n03617480\": \"kimono\",\"n03666591\": \"lighter\",\"n03670208\": \"limousine\",\"n03717622\": \"manhole cover\",\"n03720891\": \"maraca\",\"n03721384\": \"marimba\",\"n03724870\": \"mask\",\"n03775071\": \"mitten\",\"n03788195\": \"mosque\",\"n03804744\": \"nail\",\"n03837869\": \"obelisk\",\"n03840681\": \"ocarina\",\"n03854065\": \"organ\",\"n03888257\": \"parachute\",\"n03891332\": \"parking meter\",\"n03935335\": \"piggy bank\",\"n03982430\": \"billiard table\",\"n04019541\": \"hockey puck\",\"n04033901\": \"quill\",\"n04039381\": \"racket\",\"n04067472\": \"reel\",\"n04086273\": \"revolver\",\"n04099969\": \"rocking chair\",\"n04118538\": \"rugby ball\",\"n04131690\": \"salt shaker\",\"n04133789\": \"sandal\",\"n04141076\": \"saxophone\",\"n04146614\": \"school bus\",\"n04147183\": \"schooner\",\"n04179913\": \"sewing machine\",\"n04208210\": \"shovel\",\"n04235860\": \"sleeping bag\",\"n04252077\": \"snowmobile\",\"n04252225\": \"snowplow\",\"n04254120\": \"soap dispenser\",\"n04270147\": \"spatula\",\"n04275548\": \"spider web\",\"n04310018\": \"steam locomotive\",\"n04317175\": \"stethoscope\",\"n04344873\": \"couch\",\"n04347754\": \"submarine\",\"n04355338\": \"sundial\",\"n04366367\": \"suspension bridge\",\"n04376876\": \"syringe\",\"n04389033\": \"tank\",\"n04399382\": \"teddy bear\",\"n04442312\": \"toaster\",\"n04456115\": \"torch\",\"n04482393\": \"tricycle\",\"n04507155\": \"umbrella\",\"n04509417\": \"unicycle\",\"n04532670\": \"viaduct\",\"n04540053\": \"volleyball\",\"n04554684\": \"washing machine\",\"n04562935\": \"water tower\",\"n04591713\": \"wine bottle\",\"n04606251\": \"shipwreck\",\"n07583066\": \"guacamole\",\"n07695742\": \"pretzel\",\"n07697313\": \"cheeseburger\",\"n07697537\": \"hot dog\",\"n07714990\": \"broccoli\",\"n07718472\": \"cucumber\",\"n07720875\": \"bell pepper\",\"n07734744\": \"mushroom\",\"n07749582\": \"lemon\",\"n07753592\": \"banana\",\"n07760859\": \"custard apple\",\"n07768694\": \"pomegranate\",\"n07831146\": \"carbonara\",\"n09229709\": \"bubble\",\"n09246464\": \"cliff\",\"n09472597\": \"volcano\",\"n09835506\": \"baseball player\",\"n11879895\": \"rapeseed\",\"n12057211\": \"yellow lady's slipper\",\"n12144580\": \"corn\",\"n12267677\": \"acorn\"\n",
    "}\n",
    "\n",
    "# Load CLIP pre-process functions\n",
    "_, clip_preprocess = clip.load(clip_version, device)\n",
    "base_transform = v2.Compose(clip_preprocess.transforms[:3]) # resize + center crop + RGB\n",
    "preprocess = v2.Compose(clip_preprocess.transforms[3:]) # toTensor + CLIP normalization\n",
    "\n",
    "class S3ImageFolder(Dataset):\n",
    "    '''\n",
    "    Dataset which handles single images\n",
    "    '''\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get list of objects in the bucket\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        for ds_idx, item in enumerate(objects):\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "\n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((IMGNET_A_CLASSES[label], key))\n",
    "\n",
    "        # Sort classes in alphabetical order (as in ImageFolder)\n",
    "        self.classes = sorted(set(label for label, _ in self.instances))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            label, key = self.instances[idx]\n",
    "\n",
    "            # Download image from S3\n",
    "            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "\n",
    "            img_bytes = BytesIO()\n",
    "            response = self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "\n",
    "            # Open image with PIL\n",
    "            img = Image.open(img_bytes).convert(\"RGB\")\n",
    "\n",
    "            # Apply transformations if any\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n",
    "\n",
    "        return img, self.class_to_idx[label]\n",
    "\n",
    "\n",
    "    def get_image(self, idx):\n",
    "        '''\n",
    "        Returns (PIL image + label) without applying the transform\n",
    "        '''\n",
    "        try:\n",
    "            label, key = self.instances[idx]\n",
    "            img_bytes = BytesIO()\n",
    "            response = self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes)\n",
    "            # Open image with PIL\n",
    "            img = Image.open(img_bytes).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class Augmenter(Dataset):\n",
    "    '''\n",
    "    Dataset which handles augmentations\n",
    "    '''\n",
    "    def __init__(self, dataset_images, aug_component):\n",
    "        self.dataset_images = dataset_images\n",
    "        # object responsible for contructing augmentations\n",
    "        self.aug_component = aug_component\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_images) * args_data['n_aug']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Gather PIL image\n",
    "        label, key = self.dataset_images.instances[idx]\n",
    "        img_bytes = BytesIO()\n",
    "        response = self.dataset_images.s3_client.download_fileobj(Bucket=self.dataset_images.s3_bucket, Key=key, Fileobj=img_bytes)\n",
    "        img = Image.open(img_bytes).convert(\"RGB\")\n",
    "\n",
    "        # Apply augmentations\n",
    "        original_image = preprocess(base_transform(img))\n",
    "        augmentations = [self.aug_component.augment(img) for _ in range(args_data['n_aug'])]\n",
    "        augmentations =  [original_image] + augmentations\n",
    "\n",
    "        # stack augmentations together as tensors\n",
    "        augmentations = torch.stack(augmentations, dim=0)\n",
    "\n",
    "        return augmentations\n",
    "\n",
    "\n",
    "class Augmented_dataset(Dataset):\n",
    "    '''\n",
    "    Join together S3ImageFolder & Augmenter in a single dataset\n",
    "    '''\n",
    "    def __init__(self, images_dataset, augmenter):\n",
    "        self.images_dataset = images_dataset\n",
    "        self.augmenter = augmenter\n",
    "        self.classes = images_dataset.classes\n",
    "        self.is_augmenter_on = True\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.images_dataset[idx]\n",
    "        augmentations = self.augmenter[idx] if self.is_augmenter_on else torch.zeros(image.shape)\n",
    "        return image, label, augmentations, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_dataset)\n",
    "\n",
    "    def turnOFFaugs(self):\n",
    "        self.is_augmenter_on = False\n",
    "\n",
    "    def turnONaugs(self):\n",
    "        self.is_augmenter_on = True\n",
    "\n",
    "\n",
    "def get_data(dataset, perc=1, batch_size=1, test_batch_size=1):\n",
    "    '''\n",
    "    Returns a dataloader for the given dataset\n",
    "    '''\n",
    "    # sub-sample the dataset\n",
    "    size = int(perc * len(dataset))\n",
    "    # Split dataset into train, validation, and test sets\n",
    "    dataset_subset = Subset(dataset, range(size))\n",
    "    # Create DataLoader instances\n",
    "    data_loader = DataLoader(dataset_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    #random_sampler = RandomSampler(dataset, num_samples=size)\n",
    "    #data_loader = DataLoader(dataset, batch_size=batch_size, sampler=random_sampler, shuffle=False, num_workers=4)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentations ⭐\n",
    "\n",
    "We've tested with 4 metohds to augment images :\n",
    "1) **PreAugment** : applies only random crop to the image\n",
    "2) **AugMix**    [[1]](#References) [[2]](#References) : the method used in the original TPT implementation, technique which mixes randomly generated augmentations and\n",
    "uses a Jensen-Shannon loss to enforce consistency\n",
    "3) **AutoAugment** [[3]](#References) : a reinforcement learning based method which augment an image according to the one maximizing accuracy (trained on ImageNet)\n",
    "4) **DiffusionAugment** [[4]](#References) : uses a diffusion model to generate augmentations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Here we present the outcomes of applying different augmentation techniques using TPT with handcrafted prompts. The evaluation metrics include average accuracy and average loss (entropy) across the test dataset, providing insights into how each augmentation method influences the model's performance.\n",
    "\n",
    "##### N.B.\n",
    "* In the case of **DiffusionAugment** while testing we've realized <u>it is too much expensive (time wise) to generate images online during evaluation for our hardware</u>.<br>It takes around $\\sim$12 secs. for the diffusion model we've selected to perform 25 diffusion steps. Moreover, a single augmentation isn't enough to us and even downsampling the number of augmentations to generate from 64 to 10 would still be expensive (2 min. per image $\\times$ 7500 for ImageNet-A = 250 hours of runtime).<br>A work which tests the effectiveness of diffusion models combined with TPT is [**DiffTPT**](https://arxiv.org/abs/2308.06038), in which they avoid the issue of \"online generation\" by basically generating offline augmentations and store them apart ready to be used during inference. **We consider such solution not really aligned with the goal of TTA as it breaks down the whole principle of improving during inference only**. For this reason we stopped experimenting with this solution and didn't report any results (other than the code) related to it.\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "| <!-- --> | <!-- --> |\n",
    "|-|-|\n",
    "|<img src=\"imgs/augmentations_result_1.png\" width=\"650\" />|<img src=\"imgs/augmentations_result_2.png\" width=\"650\" />|\n",
    "\n",
    "</div>\n",
    "\n",
    "**Average Accuracy** (TPT - Handcrafted Prompts):\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "| Augmentation Technique | Avg Accuracy (%) |\n",
    "| ---------------------- | ---------------- |\n",
    "| RandomCrop + Flip      | 27.51             |\n",
    "| **AutoAugment**        | **30.36**         |\n",
    "| AugMix                 | 28.80             |\n",
    "\n",
    "</div>\n",
    "\n",
    "**Average Loss** (TPT - Handcrafted Prompts):\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "| Augmentation Technique | Avg Loss |\n",
    "| ---------------------- | -------- |\n",
    "| RandomCrop + Flip      | 3.02041  |\n",
    "| **AutoAugment**        | **1.89376** |\n",
    "| AugMix                 | 1.91948  |\n",
    "\n",
    "</div>\n",
    "\n",
    "From these results, we observe that <u>**AutoAugment** outperforms the other techniques</u> achieving the highest average accuracy of 30.36% and the lowest average loss of 1.89376. This indicates that AutoAugment not only improves the model's predictive accuracy but also enhances its ability to generalize by reducing the loss during inference.\n",
    "\n",
    "**RandomCrop + Flip** performs the worst in this comparison and this suggests that while basic augmentations can introduce some robustness, more advanced techniques like AutoAugment are necessary for optimizing performance in this scenario. **AugMix** shows better results than RandomCrop + Flip but still falls short compared to AutoAugment.\n",
    "\n",
    "Overall, the results highlight the importance of choosing the right augmentation strategy when working with TPT and handcrafted prompts. **AutoAugment** clearly demonstrates superior performance, suggesting that its ability to apply diverse and meaningful transformations to the input data leads to better alignment between visual and textual representations, thereby enhancing the model's accuracy and reducing its loss.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image AugmentationExamples\n",
    "\n",
    "<br>\n",
    "\n",
    "### > PreAugment - AugMix - AutoAugment\n",
    "<br>\n",
    "\n",
    "<div align=center><img src=\"imgs/augmentations.png\" width=\"1500\" /></div>\n",
    "\n",
    "<br><hr><br>\n",
    "\n",
    "### > DiffusionAugment\n",
    "<br>\n",
    "\n",
    "<div align=center><img src=\"imgs/diff_augment_ex.png\" width=\"800\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Process Explanation\n",
    "\n",
    "1. **PreAugment** : <br>\n",
    "Applies Basic transformation of RandomResizedCrop + RandomHorizontalFlip.<br>This method is **used as preliminary augmentation to both AugMix and AutoAugment** before the application of their specific augmentations\n",
    "\n",
    "2. **AugMix** : <br>\n",
    "Follows the original implementation [[2]](#References) as we've found hard to work with the one provided by PyTorch.\n",
    "\n",
    "3. **AutoAugment** : <br>\n",
    "We don't directly apply the PyTorch implementation, instead we took inspiration from AugMix in the way each augmentation is returned.<br>A factor $m$ is sampled from a $\\beta$-distribution and the resulting tensor is given by : $(m)$ $\\cdot$ AutoAugment + $(1-m)$ $\\cdot$ PreAugment input.<br>We've found this approach of constraining the output with informations coming from the original image to be much more robust than applying directly the augmentation.\n",
    "\n",
    "5. **DiffusionAugment** <br>\n",
    "Follows the implementation provided by HuggingFace [[4]](#References)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "\n",
    "def get_preaugment():\n",
    "    '''\n",
    "    PreAugment : applies only random crop to the image\n",
    "    '''\n",
    "    return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "class augMethod():\n",
    "    def __init__(self, selection, severity=1):\n",
    "        self.selection = selection\n",
    "\n",
    "        if selection == 'AugMix' :\n",
    "            self.aug_component = my_augMix(severity=severity, n_aug=args_data['n_aug'])\n",
    "        elif selection == 'AutoAugment' :\n",
    "            self.aug_component = my_AutoAugment()\n",
    "        elif selection == 'DiffusionAugment' :\n",
    "            self.aug_component = my_diffusionAugmenter()\n",
    "\n",
    "    def augment(self, img):\n",
    "        if self.selection == 'PreAugment':\n",
    "            transform = get_preaugment()\n",
    "            return preprocess(transform(img))\n",
    "\n",
    "        return self.aug_component.apply_augmentation(img)\n",
    "\n",
    "\n",
    "class my_diffusionAugmenter():\n",
    "    '''\n",
    "    Personalized instance of diffusion-based augmentation\n",
    "    uses a diffusion model to generate augmentations\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n",
    "            \"lambdalabs/sd-image-variations-diffusers\",\n",
    "            revision=\"v2.0\",\n",
    "          )\n",
    "        self.sd_pipe = self.sd_pipe.to(device)\n",
    "\n",
    "        self.base_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(\n",
    "                (224, 224),\n",
    "                interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "                antialias=False,\n",
    "                ),\n",
    "            transforms.Normalize(\n",
    "              [0.48145466, 0.4578275, 0.40821073],\n",
    "              [0.26862954, 0.26130258, 0.27577711]),\n",
    "        ])\n",
    "        args_data['n_aug'] = 4\n",
    "\n",
    "\n",
    "    def apply_augmentation(self, img):\n",
    "        src_img = img.copy()\n",
    "        x_orig = self.base_transform(src_img).to(device).unsqueeze(0)\n",
    "        out = self.sd_pipe(x_orig, guidance_scale=3, num_inference_steps=25, output_type='pil')\n",
    "        out = preprocess(base_transform(out[\"images\"][0]))\n",
    "        return out\n",
    "\n",
    "\n",
    "class my_AutoAugment():\n",
    "    '''\n",
    "    Personalized instance of AutoAugment component\n",
    "    reinforcement learning based method which augment an image according to the one maximizing accuracy (trained on ImageNet)\n",
    "    '''\n",
    "    def __init__(self, n_aug=args_data['n_aug']):\n",
    "        self.aug_component = v2.AutoAugment().to(device)\n",
    "        self.transform = [\n",
    "                self.aug_component\n",
    "            ]\n",
    "\n",
    "    def apply_augmentation(self, img):\n",
    "        preaugment = get_preaugment()\n",
    "        x_orig = preaugment(img)\n",
    "        x_processed = preprocess(x_orig)\n",
    "\n",
    "        if len(self.transform) == 0:\n",
    "            return x_processed\n",
    "\n",
    "        m = np.float32(np.random.beta(1.0, 1.0))\n",
    "        x_aug = x_orig.copy()\n",
    "        x_aug = self.transform[0](x_aug)\n",
    "        mix = m * x_processed + (1 - m) * preprocess(x_aug)\n",
    "\n",
    "        return mix\n",
    "\n",
    "\n",
    "class my_augMix():\n",
    "    '''\n",
    "    Personalized instance of AugMix component\n",
    "    '''\n",
    "    def __init__(self, severity=3, n_aug=args_data['n_aug']):\n",
    "\n",
    "        self.severity = severity\n",
    "        self.n_aug = n_aug\n",
    "\n",
    "        self.transform = [\n",
    "            self.autocontrast, self.equalize, self.posterize, self.rotate, self.solarize, self.shear_x, self.shear_y,\n",
    "            self.translate_x, self.translate_y\n",
    "        ]\n",
    "\n",
    "    # apply augmentations to a PIL image and retrun them as a list\n",
    "    def apply_augmentation(self, img):\n",
    "        preaugment = get_preaugment()\n",
    "        x_orig = preaugment(img)\n",
    "        x_processed = preprocess(x_orig)\n",
    "        if len(self.transform) == 0:\n",
    "            return x_processed\n",
    "        w = np.float32(np.random.dirichlet([1.0, 1.0, 1.0]))\n",
    "        m = np.float32(np.random.beta(1.0, 1.0))\n",
    "\n",
    "        mix = torch.zeros_like(x_processed)\n",
    "        for i in range(3):\n",
    "            x_aug = x_orig.copy()\n",
    "            for _ in range(np.random.randint(1, 4)):\n",
    "                x_aug = np.random.choice(self.transform)(x_aug, self.severity)\n",
    "            mix += w[i] * preprocess(x_aug)\n",
    "        mix = m * x_processed + (1 - m) * mix\n",
    "\n",
    "        return mix\n",
    "\n",
    "    # Utility functions\n",
    "    def int_parameter(self, level, maxval):\n",
    "        return int(level * maxval / 10)\n",
    "\n",
    "    def float_parameter(self, level, maxval):\n",
    "        return float(level) * maxval / 10.\n",
    "\n",
    "    def sample_level(self, n):\n",
    "        return np.random.uniform(low=0.1, high=n)\n",
    "\n",
    "    # AUGMENTATIONS\n",
    "\n",
    "    def autocontrast(self, pil_img, _):\n",
    "        return ImageOps.autocontrast(pil_img)\n",
    "\n",
    "    def equalize(self, pil_img, _):\n",
    "        return ImageOps.equalize(pil_img)\n",
    "\n",
    "    def posterize(self, pil_img, level):\n",
    "        level = self.int_parameter(self.sample_level(level), 4)\n",
    "        return ImageOps.posterize(pil_img, 4 - level)\n",
    "\n",
    "    def rotate(self, pil_img, level):\n",
    "        degrees = self.int_parameter(self.sample_level(level), 30)\n",
    "        if np.random.uniform() > 0.5:\n",
    "            degrees = -degrees\n",
    "        return pil_img.rotate(degrees, resample=Image.BILINEAR)\n",
    "\n",
    "    def solarize(self, pil_img, level):\n",
    "        level = self.int_parameter(self.sample_level(level), 256)\n",
    "        return ImageOps.solarize(pil_img, 256 - level)\n",
    "\n",
    "    def shear_x(self, pil_img, level):\n",
    "        level = self.float_parameter(self.sample_level(level), 0.3)\n",
    "        if np.random.uniform() > 0.5:\n",
    "            level = -level\n",
    "        return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE), Image.AFFINE, (1, level, 0, 0, 1, 0), resample=Image.BILINEAR)\n",
    "\n",
    "    def shear_y(self, pil_img, level):\n",
    "        level = self.float_parameter(self.sample_level(level), 0.3)\n",
    "        if np.random.uniform() > 0.5:\n",
    "            level = -level\n",
    "        return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE), Image.AFFINE, (1, 0, 0, level, 1, 0), resample=Image.BILINEAR)\n",
    "\n",
    "    def translate_x(self, pil_img, level):\n",
    "        level = self.int_parameter(self.sample_level(level), IMAGE_SIZE / 3)\n",
    "        if np.random.random() > 0.5:\n",
    "            level = -level\n",
    "        return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE), Image.AFFINE, (1, 0, level, 0, 1, 0), resample=Image.BILINEAR)\n",
    "\n",
    "    def translate_y(self, pil_img, level):\n",
    "        level = self.int_parameter(self.sample_level(level), IMAGE_SIZE / 3)\n",
    "        if np.random.random() > 0.5:\n",
    "            level = -level\n",
    "        return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE), Image.AFFINE, (1, 0, 0, 0, 1, level), resample=Image.BILINEAR)\n",
    "\n",
    "    # operation that overlaps with ImageNet-C's test set\n",
    "    def color(self, pil_img, level):\n",
    "        level = self.float_parameter(self.sample_level(level), 1.8) + 0.1\n",
    "        return ImageEnhance.Color(pil_img).enhance(level)\n",
    "\n",
    "    # operation that overlaps with ImageNet-C's test set\n",
    "    def contrast(self, pil_img, level):\n",
    "        level = self.float_parameter(self.sample_level(level), 1.8) + 0.1\n",
    "        return ImageEnhance.Contrast(pil_img).enhance(level)\n",
    "\n",
    "    # operation that overlaps with ImageNet-C's test set\n",
    "    def brightness(self, pil_img, level):\n",
    "        level = self.float_parameter(self.sample_level(level), 1.8) + 0.1\n",
    "        return ImageEnhance.Brightness(pil_img).enhance(level)\n",
    "\n",
    "    # operation that overlaps with ImageNet-C's test set\n",
    "    def sharpness(self, pil_img, level):\n",
    "        level = self.float_parameter(self.sample_level(level), 1.8) + 0.1\n",
    "        return ImageEnhance.Sharpness(pil_img).enhance(level)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Augmentation ⭐\n",
    "\n",
    "In this section, we introduce a novel approach for augmenting prompts using an **image captioning system**. \n",
    "\n",
    "This method aims to create more context-aware prompts compared to the standard, generic descriptions like \"a photo of a [class label].\" Our hypothesis is that captions specifically tailored to the content of the image will enhance the alignment between the image and the class labels, leading to improved model performance.\n",
    "\n",
    "### Method Overview\n",
    "\n",
    "1. **Image Captioning**: <br>\n",
    "We use the VisionEncoderDecoderModel (ViT-GPT2) [[5]](#References) to generate descriptive captions from the images. This model integrates a Vision Transformer (ViT) with GPT-2, allowing it to produce detailed captions that capture the visual content of the images.\n",
    "\n",
    "2. **KeyWords Extraction**: <br>\n",
    "After generating the caption, we utilize KeyBERT [[6]](#References) to extract the most relevant keywords or phrases from the caption. These keywords represent the key elements or subjects described in the caption.\n",
    "\n",
    "3. **Personalized Prompts composition**: <br>\n",
    "We replace the most relevant keyword in the caption with each class label from the dataset to create personalized prompts. This process generates a set of prompts specific to the content of the image and the class labels.\n",
    "\n",
    "### High Level Schema\n",
    "<center><img src=\"imgs/image_captioning_schema.png\" width=\"1500\"></center>\n",
    "\n",
    "### Results and Discussion\n",
    "\n",
    "We evaluated our proposed prompt augmentation method using an image captioning system and compared it with baseline methods. The performance was assessed on both the variants of zero-shot CLIP: CLIP-RN50 and CLIP-ViT-B/16.\n",
    "\n",
    "Below are the results of our method compared to the baseline:\n",
    "\n",
    "**Average Loss and Accuracy for Zero-Shot CLIP (CLIP-RN50):**\n",
    "\n",
    "| Method                | Avg Loss      | Avg Accuracy (%) |\n",
    "| --------------------- | ------------- | ---------------- |\n",
    "| Our Method            | 3.0781        | 19.41            |\n",
    "| Baseline              | -             | 21.83            |\n",
    "\n",
    "**Average Loss and Accuracy for Zero-Shot CLIP (CLIP-ViT-B/16):**\n",
    "\n",
    "| Method                | Avg Loss      | Avg Accuracy (%) |\n",
    "| --------------------- | ------------- | ---------------- |\n",
    "| Our Method            | 2.5711        | 42.13            |\n",
    "| Baseline              | -             | 47.87            |\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "For **CLIP-RN50**, our method achieved an average loss of 3.0781 and an average accuracy of 19.41%. In comparison, the baseline method yielded an average accuracy of 21.83%. The results indicate that our approach led to a decrease in accuracy, suggesting that the personalized prompts generated through image captioning did not provide the anticipated benefit over the standard method.\n",
    "\n",
    "For **CLIP-ViT-B/16**, our method resulted in an average loss of 2.5711 and an average accuracy of 42.13%. The baseline accuracy for CLIP-ViT-B/16 was 47.87%. The accuracy was still below the baseline, indicating that the context-aware prompts did not fully match the performance of the standard approach.\n",
    "\n",
    "#### Discussion\n",
    "\n",
    "Despite our hypothesis that contextually specific prompts would improve model performance, the **results suggest otherwise**. The personalized **prompts generated by the image captioning system did not achieve better results than the standard approaches**. The observed decrease in accuracy, highlights that this method may not have been effective in enhancing alignment between the image content and class labels in the tested configurations. Potential weaknesses to our method are discussed [**later**](#Drawbacks).\n",
    "\n",
    "This outcome provides valuable insights into the challenges of using image captions for prompt augmentation. It underscores the need for further refinement of the method and suggests that additional techniques or adjustments might be necessary to achieve the desired improvements. Future exploration could focus on optimizing the image captioning and keyword extraction processes, or integrating other innovative approaches to better leverage context-aware prompts for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyCtL2P0uAOi"
   },
   "outputs": [],
   "source": [
    "# Image captioning chain\n",
    "model_vitGpt = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# Load keyword extractor\n",
    "model_keyBert = KeyBERT()\n",
    "\n",
    "gen_kwargs = {\"max_length\": 16, \"num_beams\": 4}\n",
    "def get_captions(images):\n",
    "    '''\n",
    "    Produce captions using vit-gpt2 chain\n",
    "    '''\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    output_ids = model_vitGpt.generate(pixel_values, **gen_kwargs)\n",
    "    preds = tokenizer_gpt2.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_keywords(text) :\n",
    "    '''\n",
    "    Extract keywords from a sentence using KeyBERT\n",
    "    '''\n",
    "    keywords = model_keyBert.extract_keywords(text)\n",
    "    words = model_keyBert.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None)\n",
    "    words = [keyword[0] for keyword in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_personilized_prompt(net, dataset, idx):\n",
    "    '''\n",
    "    Produce at-hoc context caption and insert real class name\n",
    "    '''\n",
    "    img, _ = dataset.images_dataset.get_image(idx)\n",
    "    caption = get_captions([img])[0]  # feed list of PIL images to the ViT-GPT2 pipeline and produce caption\n",
    "    subject = get_keywords(caption)[0]  # get head of sentence\n",
    "    prompts = [caption.replace(subject, label) for label in dataset.classes] # substitute the head with the class label\n",
    "    prompts = [(prompt if len(prompt) <= 70 else prompt[:70]) for prompt in prompts]\n",
    "    tokenized_prompts = clip.tokenize(prompts).to(device)\n",
    "    texts_z = net.text_encoder(tokenized_prompts)\n",
    "    return texts_z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Process Explanation\n",
    "\n",
    "1. **Caption Generation** : <br>\n",
    "The first step involves generating a descriptive caption for each image using the ViT-GPT2 model. This caption provides a detailed textual representation of the visual content.\n",
    "\n",
    "2. **Keyword Extraction** : <br>\n",
    "From the generated caption, we extract the most significant keywords using KeyBERT. KeyBERT is obviously a BERT variant optimized on predicting words relevance in a given sentence.<br>Most relevant words are what we call **KeyWords**. Specifically, we're interested in knowing what is **the most relevant word among all words**\n",
    "\n",
    "3. **Prompt Personalization** : <br>\n",
    "The extracted keyword is then substituted with class labels from the dataset to create contextually relevant prompts. These prompts are designed to be more specific and better aligned with the image content, potentially improving the model’s ability to classify the image accurately.\n",
    "\n",
    "\n",
    "#### Drawbacks\n",
    "\n",
    "This implementation basically <u>delegates the handcrafted prompt design to an image captioner model</u>. Such design can potentially be harmful since :\n",
    "* Performances are dependent on an secondary supervising model (ViT-GPT2 chain in this case) which is detatched from the rest.\n",
    "* If the input image is noisy, the produced caption will also probably be noisy, making the inference even harder than using a more generic prompt like \"a photo of a { label }\"<br>As we're evaluating performances on an noisy dataset such as ImageNet-A this might be the most relevant aspect\n",
    "* As stated by OpenAI itself, CLIP is very sensitive to wording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUAk-ZQbKa1P"
   },
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gXGoJEmnJ-aJ"
   },
   "outputs": [],
   "source": [
    "class EntropyLoss(nn.Module):\n",
    "    '''\n",
    "    Unsupervised entropy loss function    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(EntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return -torch.sum(x * torch.log(x + 1e-9), dim=-1)\n",
    "\n",
    "def get_loss_function() :\n",
    "    return EntropyLoss()\n",
    "\n",
    "def get_optimizer(model, lr=0.005, wd=0.0005, momentum=0.9):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xst3mxDPZBjL"
   },
   "source": [
    "### CoOp prompt Learner + CLIP adapted text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2tvKVRwXZPSt"
   },
   "outputs": [],
   "source": [
    "args_CoOp = {\n",
    "    \"n_ctx\": 4,\n",
    "    \"ctx_init\": \"a photo of a\",\n",
    "    \"class_token_position\": \"end\",\n",
    "    \"csc\": False,\n",
    "}\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        # prompts : ones we learn\n",
    "        # tokenized_prompts : from original input : for eot\n",
    "        x = prompts + self.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classes, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        n_cls = len(classes)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        # Use given words to initialize context vectors\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            if csc:\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
    "            else:\n",
    "                print(\"Initializing a generic context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        # These are the `prompts` we want to optimize\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        classes = [name.replace(\"_\", \" \") for name in classes]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classes]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classes]\n",
    "\n",
    "        # print(\"+++\")\n",
    "        # print(\"Prompts:\")\n",
    "        # for p in prompts:\n",
    "        #     print(p)\n",
    "        # print(\"+++\")\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "\n",
    "        # If CoOp, expand the ctx for all classes\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix,  # (n_cls, 1, dim)\n",
    "                    ctx,     # (n_cls, n_ctx, dim)\n",
    "                    suffix,  # (n_cls, *, dim)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
    "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,     # (1, 1, dim)\n",
    "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "                        class_i,      # (1, name_len, dim)\n",
    "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "                        suffix_i,     # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i = ctx[i : i + 1, :, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        class_i,   # (1, name_len, dim)\n",
    "                        ctx_i,     # (1, n_ctx, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy-hexu9Kd_8"
   },
   "source": [
    "# TPT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ThxsOu2G2Bca"
   },
   "outputs": [],
   "source": [
    "class TPT(nn.Module) :\n",
    "    '''\n",
    "    Model Class of TPT\n",
    "    '''\n",
    "    def __init__(self, clip_model, classes, prompts=None, only_CLIP=True, enable_CoOp=False):\n",
    "        super().__init__()\n",
    "        self.enable_CoOp = enable_CoOp # if set True, the model uses TPT + CoOp\n",
    "        self.only_CLIP = only_CLIP     # if set True, the model uses CLIP only (TPT part is not performed)\n",
    "\n",
    "        if not enable_CoOp or only_CLIP :\n",
    "            # Generate text embeddings\n",
    "            tokenized_prompts = clip.tokenize(prompts).to(device)\n",
    "            self.texts_z = clip_model.encode_text(tokenized_prompts)\n",
    "            self.text_encoder = clip_model.encode_text\n",
    "        else :\n",
    "            # Setup CoOp prompt learner\n",
    "            self.prompt_learner = PromptLearner(clip_model, classes, args_CoOp['n_ctx'], args_CoOp['ctx_init'], args_CoOp['class_token_position'], args_CoOp['csc'])\n",
    "            # References to image + text encoders\n",
    "            self.text_encoder = TextEncoder(clip_model)\n",
    "\n",
    "        # References to image + text encoders\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "\n",
    "\n",
    "    def forward (self, inputs):\n",
    "        if self.enable_CoOp :\n",
    "            # Embed prompts\n",
    "            prompts = self.prompt_learner()\n",
    "            tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "            self.texts_z = self.text_encoder(prompts, tokenized_prompts)\n",
    "\n",
    "        # Embed augmentations\n",
    "        images_z = self.image_encoder(inputs)\n",
    "\n",
    "        # L2 norm to tensors\n",
    "        images_z = images_z / torch.linalg.vector_norm(images_z, keepdim=True, dim=-1)\n",
    "        texts_z = self.texts_z / torch.linalg.vector_norm(self.texts_z, keepdim=True, dim=-1)\n",
    "\n",
    "        # Compute logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        output = (logit_scale * images_z @ texts_z.T).softmax(dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def testTimeAdapt(self, clip_output, loss_function):\n",
    "        '''\n",
    "        Apply augmentation selection based on entropies and average the histograms\n",
    "        '''\n",
    "        # Entropy for each augmentation\n",
    "        aug_losses = loss_function(clip_output)\n",
    "        # retain top 'retain_aug_perc'%, ordered by entropy\n",
    "        output = torch.index_select(clip_output, 0, torch.argsort(aug_losses, descending=False))[:int(np.ceil(args_data['n_aug'] * args_data['retain_aug_perc']))]\n",
    "        # average\n",
    "        output = torch.mean(output, dim=0)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy-hexu9Kd_8"
   },
   "source": [
    "## TRAIN / TEST functions\n",
    "\n",
    "We've separated eval loops to have cleaner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VS6xzY4KALfb"
   },
   "outputs": [],
   "source": [
    "def eval_loop(net, loss_function, test_loader, dataset, enable_personilized_prompt=False):\n",
    "    '''\n",
    "    Perform Zero-Shot clip / TPT\n",
    "    '''\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    desc = \"Zero Shot CLIP\" if net.only_CLIP else \"TPT - handcrafted prompts\" \n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test set\n",
    "        for batch_idx, (inputs, targets, augmentations, idx) in tqdm(enumerate(test_loader), desc=desc, position=0, leave=True, total=len(test_loader)):\n",
    "            # Load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if not net.only_CLIP :\n",
    "                augmentations = torch.squeeze(augmentations, axis=0)\n",
    "                augmentations = augmentations.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            net.texts_z = get_personilized_prompt(net, dataset, idx) if enable_personilized_prompt else net.texts_z\n",
    "            output = net(inputs) if net.only_CLIP else net(augmentations)\n",
    "            # Apply augmentation selection based on entropies and average the histograms\n",
    "            output = output if net.only_CLIP else net.testTimeAdapt(output, loss_function)\n",
    "            # Loss computation\n",
    "            loss = loss_function(output)\n",
    "\n",
    "            # Compute matching\n",
    "            _, predicted = output.max(dim=-1)\n",
    "\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "            samples += inputs.shape[0]\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "    return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
    "\n",
    "\n",
    "def fewShot_eval_loop(net, loss_function, optimizer, train_loader):\n",
    "    '''\n",
    "    Perform few shot inference with TPT + CoOp\n",
    "    '''\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    optim_state = deepcopy(optimizer.state_dict())\n",
    "    prompt_learner_state = deepcopy(net.prompt_learner.state_dict())\n",
    "\n",
    "    #net.train()\n",
    "    for batch_idx, (inputs, targets, augmentations, _) in tqdm(enumerate(train_loader), desc=\"TPT + CoOp\", position=0, leave=True, total=len(train_loader)):\n",
    "        net.train()\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        augmentations = torch.squeeze(augmentations, axis=0)  # [1, n_aug, 3, 224, 224] -> [n_aug, 3, 224, 224]\n",
    "        augmentations = augmentations.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = net(augmentations)\n",
    "\n",
    "        # ================================= TPT computation ================================= #\n",
    "\n",
    "        # Apply augmentation selection based on entropies and average the histograms\n",
    "        output = net.testTimeAdapt(output, loss_function)\n",
    "        # Compute Loss and update weights\n",
    "        loss = loss_function(output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ================================= INFERENCE ================================= #\n",
    "\n",
    "        # Forward pass again to compute accuracy\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward again + TPT computation\n",
    "            output = net(augmentations)\n",
    "            output = net.testTimeAdapt(output, loss_function)\n",
    "\n",
    "            loss = loss_function(output)\n",
    "            _, predicted = output.max(dim=-1)\n",
    "            # Calculate scores\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "            samples += inputs.shape[0]\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "        # reset prompt learner & optimizer to its original state\n",
    "        net.prompt_learner.load_state_dict(prompt_learner_state)\n",
    "        optimizer.load_state_dict(optim_state)\n",
    "\n",
    "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW4FaJsbKLBL"
   },
   "source": [
    "## RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"lr\": 0.005,\n",
    "    \"wd\": 0.0005,\n",
    "    \"momentum\": 0.9,\n",
    "    \"n_epochs\": 1,\n",
    "}\n",
    "\n",
    "# Set WanDB logger to track resuls\n",
    "logger_on = False\n",
    "\n",
    "if logger_on :\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"TPT-CoOp\",\n",
    "    \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"epochs\": params[\"n_epochs\"],\n",
    "            \"learning_rate\": params[\"lr\"],\n",
    "            \"momentum\": params[\"momentum\"],\n",
    "            \"weight decay\": params[\"wd\"],\n",
    "            \"number of augmentations\": args_data[\"n_aug\"],\n",
    "            \"dataset\": args_data[\"dataset\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_augs = {\n",
    "    'img': 'AutoAugment', # PreAugment - AugMix  - AutoAugment - DiffusionAugment\n",
    "    'prompt' : False,   # (True/False) to enable / disable prompt augmentation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nedFAUtQK8NK"
   },
   "outputs": [],
   "source": [
    "def main(flg=(False, False, False)):\n",
    "    # Flags to enable/disable models when running the function\n",
    "    flag_zeroShot, flag_TPT_handCrafted, flag_TPT_CoOp = flg\n",
    "\n",
    "    # Load CLIP pre-trained weights\n",
    "    clip_model, clip_preprocess = clip.load(clip_version)\n",
    "    clip_model = clip_model.float()\n",
    "    clip_model = clip_model.to(device)\n",
    "\n",
    "    # Setup images dataset\n",
    "    ds_images = S3ImageFolder(args_data[\"dataset\"], transform=clip_preprocess)\n",
    "    # Setup aumentated dataset\n",
    "    aug_component = augMethod(params_augs['img'])\n",
    "    ds_augmentations = Augmenter(ds_images, aug_component)\n",
    "    # Merge the 2 as a single dataset\n",
    "    dataset = Augmented_dataset(ds_images, ds_augmentations)\n",
    "\n",
    "    # Turn off gradients in CLIP's image & text encoders\n",
    "    for _, param in clip_model.named_parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    # Handcrafted prompts to use\n",
    "    handcrafted_prompts = [f\"a photo of a {label}\" for label in dataset.classes]\n",
    "\n",
    "    if flag_zeroShot :\n",
    "        model_TPT_clipOnly = TPT(clip_model, ds_images.classes, prompts=handcrafted_prompts, only_CLIP=True, enable_CoOp=False).to(device)\n",
    "    \n",
    "    if flag_TPT_handCrafted:\n",
    "        model_TPT_handCrafted = TPT(clip_model, ds_images.classes, prompts=handcrafted_prompts, only_CLIP=False, enable_CoOp=False).to(device)\n",
    "\n",
    "    if flag_TPT_CoOp:\n",
    "        model_TPT_CoOp = TPT(clip_model, ds_images.classes, only_CLIP=False, enable_CoOp=True).to(device)\n",
    "        for name, param in model_TPT_CoOp.named_parameters():\n",
    "            if \"prompt_learner\" not in name:\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "    # ====================================== Run Settings ====================================== \n",
    "\n",
    "    loss_function = get_loss_function()\n",
    "    # Define dataloader\n",
    "    perc = 1  # percentage of the dataset to use\n",
    "    data_loader = get_data(dataset, perc=perc, batch_size=1)\n",
    "\n",
    "    for ep in range(params[\"n_epochs\"]):\n",
    "        # Runs Zero-Shot evaluation\n",
    "        if flag_zeroShot :\n",
    "            dataset.turnOFFaugs()\n",
    "            avg_loss, avg_acc = eval_loop(model_TPT_clipOnly, loss_function, data_loader, dataset, enable_personilized_prompt=params_augs['prompt'])\n",
    "            print(f\"   Avg loss : {avg_loss}\")\n",
    "            print(f\"   Avg accuracy : {avg_acc}\\n\")\n",
    "            if logger_on :\n",
    "                wandb.log({\"Avg loss (zero-shot CLIP)\": avg_loss, \"Avg accuracy (zero-shot CLIP)\": avg_acc})\n",
    "\n",
    "        if flag_TPT_handCrafted :\n",
    "            dataset.turnONaugs()\n",
    "            avg_loss, avg_acc = eval_loop(model_TPT_handCrafted, loss_function, data_loader, dataset, enable_personilized_prompt=params_augs['prompt'])\n",
    "            print(f\"   Avg loss : {avg_loss}\")\n",
    "            print(f\"   Avg accuracy : {avg_acc}\\n\")\n",
    "            if logger_on :\n",
    "                wandb.log({\"Avg loss (TPT - handcrafted prompts)\": avg_loss, \"Avg accuracy (TPT - handcrafted prompts)\": avg_acc})\n",
    "\n",
    "        if flag_TPT_CoOp :\n",
    "            dataset.turnONaugs()\n",
    "            optimizer = get_optimizer(model_TPT_CoOp, lr=params[\"lr\"])\n",
    "            avg_loss, avg_acc = fewShot_eval_loop(model_TPT_CoOp, loss_function, optimizer, data_loader)\n",
    "            print(f\"   Avg loss : {avg_loss}\")\n",
    "            print(f\"   Avg accuracy : {avg_acc}\\n\")\n",
    "            if logger_on :\n",
    "                wandb.log({\"Avg loss (TPT + CoOp)\": avg_loss, \"Avg accuracy (TPT + CoOp)\": avg_acc})\n",
    "\n",
    "    # end run\n",
    "    if logger_on :\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "0FnnETEsrmW9",
    "outputId": "cf68701d-3b03-4cae-f608-f28030a68ae4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "runs the evaluation procedure with specified models or methods.\n",
    "\n",
    "Flags:\n",
    "- `flag_zeroShot`: If True, the zero-shot model will be used.\n",
    "- `flag_TPT_handCrafted`: If True, Test-Time Prompt Tuning with handcrafted prompts will be used.\n",
    "- `flag_TPT_CoOp`: If True, Test-Time Prompt Tuning with CoOp will be used.\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Flags to enable/disable models when running the function\n",
    "# { flag_zeroShot, flag_TPT_handCrafted, flag_TPT_CoOp }\n",
    "flags = (True, False, False)\n",
    "main(flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results obtained with TPT on ImageNet-A ⭐\n",
    "By implementing TPT on ImageNet-A we verified the results of the original paper.\n",
    "\n",
    "Here are our results using AugMix:\n",
    "\n",
    "| Method                    | Avg Accuracy | Avg Loss |\n",
    "| ------------------------- | ------------ | -------- |\n",
    "| CLIP-RN50 (zero-shot)     | 21.88        | 2.32929  |\n",
    "| TPT (handcrafted prompts) | 28.8         | 1.91948  |\n",
    "| TPT + CoOp                | 29.41333     | 1.89968  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions ⭐\n",
    "\n",
    "The primary aim of this project was to implement a Test-Time Adaptation (TTA) solution by focusing on improving the predictive capabilities of pre-trained neural networks on unseen test samples. Our approach involved implementing Test-Time Prompt Tuning (TPT) with a few-shot prompt tuning strategy called CoOp and experimenting with various image augmentation techniques.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Evaluation of Augmentation Techniques** : <br>\n",
    "We assessed several image augmentation techniques using TPT with handcrafted prompts. The results demonstrated that **AutoAugment** significantly outperforms other methods, indicating that it provides the most effective transformations, enhancing the model’s performance and generalization ability. AugMix performed better than RandomCrop + Flip but still fell short compared to AutoAugment.\n",
    "\n",
    "2. **Prompt Augmentation Using Image Captioning** : <br>\n",
    "We introduced a novel method for generating context-aware prompts using image captioning. Although our hypothesis was that contextually specific captions would improve model alignment and performance, the results did not meet expectations. For both **CLIP-RN50** and **CLIP-ViT-B/16**, the personalized prompts generated by the image captioning system led to a decrease in accuracy compared to standard methods.\n",
    "\n",
    "3. **Results on ImageNet-A** : <br>\n",
    "Our implementation of TPT on ImageNet-A validated the results from the original paper. \n",
    "\n",
    "**Conclusions** :\n",
    "\n",
    "Overall, this project highlights the importance of selecting appropriate augmentation techniques and prompt tuning strategies to optimize model performance. While AutoAugment proved to be the most effective augmentation strategy, the image captioning-based prompt augmentation did not deliver the expected improvements. This underscores the need for further exploration and refinement of prompt augmentation methods. Future work could focus on optimizing image captioning techniques, experimenting with other innovative methods, or combining multiple strategies to better align text and visual representations for improved model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [1] [AugMix](https://pytorch.org/vision/main/generated/torchvision.transforms.AugMix.html)\n",
    "- [2] [AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty](https://arxiv.org/abs/1912.02781)\n",
    "- [3] [AutoAugment](https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html)\n",
    "- [4] [DiffusionAugment](https://huggingface.co/lambdalabs/sd-image-variations-diffusers)\n",
    "- [5] [ViT-GPT2](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)\n",
    "- [6] [KeyBERT](https://github.com/MaartenGr/KeyBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW4FaJsbKLBL"
   },
   "source": [
    "# Augmentations PlayGround ⭐\n",
    "In this section it is possible to play around with the augmentation methods we've implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose in {0 - 7499}\n",
    "input_idx = 7\n",
    "\n",
    "# Compute\n",
    "dataset = S3ImageFolder(args_data[\"dataset\"], transform=None)\n",
    "img, label = dataset.get_image(input_idx)\n",
    "\n",
    "og_caption = get_captions([img])[0]\n",
    "print(f\"caption : {og_caption}\")\n",
    "print(f\"=>   label: {label}\")\n",
    "subject = get_keywords(og_caption)[0]\n",
    "print(f\"=>   KeyWord: {subject}\")\n",
    "caption = og_caption.replace(subject, label)\n",
    "print(f\"\\noutput prompt : {caption}\\n\")\n",
    "\n",
    "# Display\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation : PreAugment - AugMix - AutoAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose in {0 - 7499}\n",
    "input_idx = 99\n",
    "# Number of augmentations per type\n",
    "num_augmentations = 7\n",
    "# regulates aggressiveness of augmentations (positive integer)\n",
    "severity = 3\n",
    "\n",
    "# Compute\n",
    "ds_images = S3ImageFolder(args_data[\"dataset\"], transform=None)\n",
    "img, label = ds_images.get_image(input_idx)\n",
    "\n",
    "aug_1 = augMethod('PreAugment', severity=severity)\n",
    "aug_2 = augMethod('AugMix', severity=severity)\n",
    "aug_3 = augMethod('AutoAugment', severity=severity)\n",
    "augmenter_names = [\"PreAugment\", \"AugMix\", \"AutoAugment\"]\n",
    "\n",
    "aug1 = aug_1.augment(img)\n",
    "aug2 = aug_2.augment(img)\n",
    "aug3 = aug_3.augment(img)\n",
    "\n",
    "invNorm = transforms.Compose([\n",
    "    transforms.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.26862954, 1/0.26130258, 1/0.27577711 ]),\n",
    "    transforms.Normalize(mean = [ -0.48145466, -0.4578275, -0.40821073 ], std = [ 1., 1., 1. ])\n",
    "])\n",
    "tensor_to_pil_img = transforms.ToPILImage()\n",
    "\n",
    "augmented_images_1 = [tensor_to_pil_img(invNorm(aug_1.augment(img))) for _ in range(num_augmentations)]\n",
    "augmented_images_2 = [tensor_to_pil_img(invNorm(aug_2.augment(img))) for _ in range(num_augmentations)]\n",
    "augmented_images_3 = [tensor_to_pil_img(invNorm(aug_3.augment(img))) for _ in range(num_augmentations)]\n",
    "\n",
    "# Display\n",
    "images = augmented_images_1 + augmented_images_2 + augmented_images_3\n",
    "augmentation_titles = [\"PreAugment\", \"AugMix\", \"AutoAugment\"]\n",
    "fig, axes = plt.subplots(4, num_augmentations, figsize=(20, 15))\n",
    "\n",
    "for ax in axes[0]:\n",
    "    ax.axis('off')\n",
    "\n",
    "mid_col = num_augmentations // 2\n",
    "axes[0, mid_col].imshow(img.resize((224, 224)))  \n",
    "axes[0, mid_col].set_title('Original', fontsize=16, pad=20)\n",
    "\n",
    "for row in range(3):  \n",
    "    for col in range(num_augmentations):\n",
    "        idx = row * num_augmentations + col\n",
    "        axes[row + 1, col].imshow(images[idx])\n",
    "        axes[row + 1, col].axis('off')\n",
    "    axes[row + 1, mid_col].set_title(augmentation_titles[row], fontsize=16, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('imgs/augmentations.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation : DiffusionAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this only once\n",
    "augmenter = augMethod('DiffusionAugment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose in {0 - 7499}\n",
    "input_idx = 7001\n",
    "\n",
    "# Compute\n",
    "ds_images = S3ImageFolder(args_data[\"dataset\"], transform=None)\n",
    "img, label = ds_images.get_image(input_idx)\n",
    "\n",
    "invNorm = transforms.Compose([\n",
    "    transforms.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.26862954, 1/0.26130258, 1/0.27577711 ]),\n",
    "    transforms.Normalize(mean = [ -0.48145466, -0.4578275, -0.40821073 ], std = [ 1., 1., 1. ])\n",
    "])\n",
    "tensor_to_pil_img = transforms.ToPILImage()\n",
    "\n",
    "aug = tensor_to_pil_img(invNorm(augmenter.augment(img)))\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(img.resize((224, 224)))\n",
    "ax[0].set_title(\"Original Image\", fontsize=16, pad=20)\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(aug)\n",
    "ax[1].set_title(\"Generated Image\", fontsize=16, pad=20)\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "#plt.savefig('imgs/diff_augment_ex.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
